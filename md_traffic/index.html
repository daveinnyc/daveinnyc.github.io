<html>
<head>
<title>Test</title>
<meta name="description" content="Notes on working with MD traffic data">
<meta name="keywords" content="data.gov,traffic">
<meta name="author" content="Dave Cuthbert">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" type="text/css" href="../shared.css">
<link rel="stylesheet" type="text/css" href="md-traffic-style.css">
</head>
<body>
<div class="hero-image">
    <div class="hero-text">
       <h1 id="h1-hero">Notes on working with MD traffic data</h1>
    </div>
 </div>
<h1>More questions than answers</h1>
<p><a href="https://catalog.data.gov/datasethttps://catalog.data.gov/dataset">Data.gov</a> has a catalog of over a 160,000 data sets that are available for anyone to work with. The state Maryland contributed a collection of <a href="https://catalog.data.gov/dataset/traffic-violations-56dda/resource/450018e7-f6c0-43fd-b5c9-a83de293b206">traffic violation data</a>. This data looked like it would be a useful tool to shed light on a commonly held belief, that more tickets are issued at the end of the month in order for officers to 'meet quota'. The MD Traffic Data did shed light on that question, but it also raised a few more.</p>

<h2>Getting Started</h2>       
<p>Data.gov provides a little information about each dataset. In this case the <a href="https://data.montgomerycountymd.gov/api/views/4mse-ku6q/rows.csv?accessType=DOWNLOAD">CSV file</a> reportedly "contains traffic violation information from all electronic traffic violations issued in the County".
<br><a target="_blank" href="images/data-source-info-page.jpg"><img src="images/data-source-info-page.jpg" alt="source information page"></a>
</p>

<p>After downloading and renaming the file the next step was to check it's contents and load it up into an SQLite database.</p>
<p>It is about 375MB in size has just under 1.1 million rows, so lots of data to work with! The linux <code>head</code> command revealed a header row and some variety in the data encoded in the file.
<br><a target="_blank" href="images/head-output.jpg"><img src="images/head-output.jpg" alt="output of head command"></a>
</p> 
<p>Running a few checks with <code>grep</code> highlighted some problems with the data. The file was uploaded in 2015, but the year field contained a thousands of entries with bad dates. There were missing dates, early dates and late dates. Rather than drop rows with improbable dates, where clauses were added to the SQL queries to restrict the results to dates that were at least plausible.
<br><a target="_blank" href="images/bad-data-year.jpg"><img src="images/bad-data-year.jpg" alt="grep for bad dates"></a></p>
<p>There were other problems with the raw data. This was a CSV file but commas were used within fields as well and as a field seperator. In some instances the sub-agency (usually a geographic designation) had a comma in its name. Other fields occasionally had internal commas. When present, the geographic coordinates were given as "(latitude, longitude)"
<br><a target="_blank" href="images/format-problems.jpg"><img src="images/format-problems.jpg" alt="grep for format problems"></a></p>
<p>More troubling than the inconsitent use of internal commas, many rows were missing significant numbers of fields altogether. Rather than trying to guess what data was present in these cases, the rows with missing fields were just dropped. Here is the <a href="https://github.com/daveinnyc/various/blob/master/data-analysis/fix-and-load.py">script</a> that was used to set up the database, clean the incoming lines from the CSV file, and then insert the rows into the database. After cleaning up the file, all but 1818 rows (about .1%) were successfully imported into the database.
<br><a target="_blank" href="images/errors-v-total-lines.jpg"><img src="images/errors-v-total-lines.jpg" alt="parsing errors"></a></p>
</p>

<div id="pagenav">
         <a class="navlink"  href="../index.html">Home</a>&nbsp;&nbsp;&nbsp;&nbsp;
         <a class="navlink" href="page..02.html">Next</a>
</div>   
</body>
</html>

